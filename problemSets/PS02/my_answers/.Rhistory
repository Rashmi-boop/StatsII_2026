#####################
# load libraries
# set wd
# clear global .envir
#####################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:", search()))==1, TRUE, FALSE)]
package.list <- setdiff(package.list, basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package,  character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
# here is where you load any necessary packages
# ex: stringr
# lapply(c("stringr"),  pkgTest)
lapply(c(),  pkgTest)
# set wd for current folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
ks_test_normal <- function(data, mean = 0, sd = 1) {
# Sample size
n <- length(data)
# Create empirical CDF
ECDF <- ecdf(data)
empiricalCDF <- ECDF(data)
# Theoretical Normal CDF evaluated at data
theoreticalCDF <- pnorm(data, mean = mean, sd = sd)
# KS test statistic
D <- max(abs(empiricalCDF - theoreticalCDF))
# Kolmogorov distribution approximation
# p-value ≈ 2 * sum_{k=1}^∞ (-1)^(k-1) * exp(-2 k^2 D^2 n)
k <- 1:100
p_value <- 2 * sum((-1)^(k-1) * exp(-2 * k^2 * D^2 * n))
p_value <- max(min(p_value, 1), 0)
return(list(
D_statistic = D,
p_value = p_value
))
}
set.seed(123)
# Generate 1000 Cauchy observations
data <- rcauchy(1000, location = 0, scale = 1)
# Run KS test against standard normal
result <- ks_test_normal(data, mean = 0, sd = 1)
result
ks_test_normal <- function(data, mean = 0, sd = 1) {
###############################################################
# One-Sample Kolmogorov–Smirnov Test
# -------------------------------------------------------------
# This function tests whether the empirical distribution of
# the observed data matches a Normal(mean, sd^2) distribution.
#
# H0: The data follow a Normal(mean, sd^2) distribution
# H1: The data do NOT follow that Normal distribution
#
# The test statistic:
# D = max | F_n(x) - F(x) |
# where:
#   F_n(x) = empirical CDF
#   F(x)   = theoretical Normal CDF
###############################################################
# The distribution of the KS statistic depends on n.
# We will need n when computing the p-value.
n <- length(data)
# Create empirical CDF
# ecdf(data) creates a FUNCTION that computes:
# F_n(x) = (# of observations <= x) / n
# This is the empirical distribution function (EDF).
ECDF <- ecdf(data)
# This gives us F_n(x_i) for each observation x_i.
# The result is a vector of cumulative probabilities.
empiricalCDF <- ECDF(data)
# pnorm(x, mean, sd) computes:
# F(x_i) = P(X <= x_i) assuming X ~ Normal(mean, sd^2)
# This gives us the theoretical cumulative probabilities.
# Theoretical Normal CDF evaluated at data
theoreticalCDF <- pnorm(data, mean = mean, sd = sd)
# KS test statistic
# For each data point, compute the absolute difference:
# |F_n(x_i) - F(x_i)|
# The KS statistic D is the maximum of these differences.
# Geometrically: this is the largest vertical distance
# between the empirical CDF and theoretical CDF.
D <- max(abs(empiricalCDF - theoreticalCDF))
# Kolmogorov distribution approximation
# p-value ≈ 2 * sum_{k=1}^∞ (-1)^(k-1) * exp(-2 k^2 D^2 n)
# The asymptotic p-value formula is:
#
# P(D_n >= d) ≈ 2 * sum_{k=1}^∞ (-1)^(k-1) exp(-2 k^2 d^2 n)
#
# Since we cannot sum to infinity, we approximate using
# the first 100 terms. The exponential terms decay very fast,
# so 100 terms is more than sufficient.
k <- 1:100
p_value <- 2 * sum((-1)^(k-1) * exp(-2 * k^2 * D^2 * n))
# Due to floating-point rounding, the computed value may
# slightly exceed 1 or drop below 0. This forces it into
# the valid probability range [0,1].
p_value <- max(min(p_value, 1), 0)
# Return both:
#   - D statistic
#   - p-value
#
# The result is returned as a named list for clarity.
return(list(
D_statistic = D,
p_value = p_value
))
}
set.seed(123)
# Generate 1000 Cauchy observations
data <- rcauchy(1000, location = 0, scale = 1)
# Run KS test against standard normal
result <- ks_test_normal(data, mean = 0, sd = 1)
result
set.seed(123)
# Generate data
data <- data.frame(x = runif(200, 1, 10))
data$y <- 2.75 * data$x + rnorm(200, 0, 1.5)
# OLS objective function (sum of squared residuals)
# Ordinary Least Squares (OLS) chooses beta to minimize:
#
#   SSR(beta) = Σ (y_i - beta * x_i)^2
#
# This is called the Sum of Squared Residuals (SSR).
#
# Instead of using the closed-form OLS solution,
# we will numerically minimize this function using BFGS.
#
# The function below takes:
#   beta = parameter to estimate
#   x    = regressor
#   y    = dependent variable
#
# and returns the SSR value.
# We now use optim() to minimize the SSR function.
#
# method = "BFGS" specifies a quasi-Newton algorithm.
#
# BFGS:
# - Uses gradient-based updates
# - Approximates the Hessian matrix
# - Iteratively moves toward the minimum
#
# Since the OLS objective function is quadratic and convex,
# it has a unique global minimum.
ols_objective <- function(beta, x, y) {
sum((y - beta * x)^2)
}
# Estimate using BFGS
optim_results <- optim(
par = 0,
fn = ols_objective,
x = data$x,
y = data$y,
method = "BFGS"
)
# Extract estimated coefficient from BFGS
beta_bfgs <- optim_results$par
# The lm() function computes the OLS estimator using
# the closed-form matrix solution:
#
#   β̂ = (X'X)^(-1) X'y
#
# Because the true model has no intercept,
# we specify: y ~ 0 + x
# Estimate using lm()
lm_results <- lm(y ~ 0 + x, data = data)
# Extract coefficient estimate from lm()
beta_lm <- coef(lm_results)
# Compare estimates
beta_bfgs
beta_lm
#####################
# load libraries
# set wd
# clear global .envir
#####################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:", search()))==1, TRUE, FALSE)]
package.list <- setdiff(package.list, basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package,  character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
# here is where you load any necessary packages
# ex: stringr
# lapply(c("stringr"),  pkgTest)
lapply(c(),  pkgTest)
# set wd for current folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
#####################
# Problem 1
#####################
# load data
load(url("https://github.com/ASDS-TCD/StatsII_2026/blob/main/datasets/climateSupport.RData?raw=true"))
climate_data <- load(url("https://github.com/ASDS-TCD/StatsII_2026/blob/main/datasets/climateSupport.RData?raw=true"))
# load data
climate_data <- load(url("https://github.com/ASDS-TCD/StatsII_2026/blob/main/datasets/climateSupport.RData?raw=true"))
mod_add <- glm(
choice ~ countries + sanctions,
data   = climate_data,
family = binomial(link = "logit")
)
climate_data <- load(url("https://github.com/ASDS-TCD/StatsII_2026/blob/main/datasets/climateSupport.RData?raw=true"))
ls()
climate_data <- load(url("https://github.com/ASDS-TCD/StatsII_2026/blob/main/datasets/climateSupport.RData?raw=true"))
ls()
class(climateSupport)
# load data
climate_data <- load(url("https://github.com/ASDS-TCD/StatsII_2026/blob/main/datasets/climateSupport.RData?raw=true"))
ls()
class(climateSupport)
mod_add <- glm(
choice ~ countries + sanctions,
data   = climateSupport,
family = binomial(link = "logit")
)
summary(mod_add)
beta <- coef(all)
beta <- coef(all)
logit_add <- glm(
choice ~ countries + sanctions,
data   = climateSupport,
family = binomial(link = "logit")
)
summary(logit_add)      # this replaces summary(all) from part 1
lp_160_5  <- predict(logit_add, newdata = data.frame(
countries = "160 of 192",
sanctions = "5%"
), type = "link")
lp_160_15 <- predict(logit_add, newdata = data.frame(
countries = "160 of 192",
sanctions = "15%"
), type = "link")
log_OR_160_5_to_15 <- lp_160_15 - lp_160_5
OR_160_5_to_15     <- exp(log_OR_160_5_to_15)
OR_160_5_to_15
lp_20_5 <- predict(all, newdata = data.frame(
countries = "20 of 192",
sanctions = "5%"
), type = "link")
lp_160_5  <- predict(logit_add, newdata = data.frame(
countries = "20 of 192",
sanctions = "5%"
), type = "link")
lp_160_15 <- predict(logit_add, newdata = data.frame(
countries = "20 of 192",
sanctions = "15%"
), type = "link")
log_OR_20_5_to_15 <- lp_20_15 - lp_20_5
lp_120_5  <- predict(logit_add, newdata = data.frame(
countries = "20 of 192",
sanctions = "5%"
), type = "link")
lp_120_15 <- predict(logit_add, newdata = data.frame(
countries = "20 of 192",
sanctions = "15%"
), type = "link")
log_OR_20_5_to_15 <- lp_20_15 - lp_20_5
# build two hypothetical cases and look at linear predictor difference
lp_120_5  <- predict(logit_add, newdata = data.frame(
countries = "20 of 192",
sanctions = "5%"
), type = "link")
lp_120_15 <- predict(logit_add, newdata = data.frame(
countries = "20 of 192",
sanctions = "15%"
), type = "link")
log_OR_120_5_to_15 <- lp_120_15 - lp_120_5
OR_120_5_to_15     <- exp(log_OR_120_5_to_15)
######################
# Problem 2b
######################
# build two hypothetical cases and look at linear predictor difference
lp_120_5  <- predict(logit_add, newdata = data.frame(
countries = "20 of 192",
sanctions = "5%"
), type = "link")
lp_120_15 <- predict(logit_add, newdata = data.frame(
countries = "20 of 192",
sanctions = "15%"
), type = "link")
log_OR_120_5_to_15 <- lp_120_15 - lp_120_5
OR_120_5_to_15     <- exp(log_OR_120_5_to_15)
OR_120_5_to_15
lp_20_5  <- predict(logit_add, newdata = data.frame(
countries = "20 of 192",
sanctions = "5%"
), type = "link")
lp_20_15 <- predict(logit_add, newdata = data.frame(
countries = "20 of 192",
sanctions = "15%"
), type = "link")
log_OR_20_5_to_15 <- lp_20_15 - lp_20_5
OR_20_5_to_15     <- exp(log_OR_20_5_to_15)
OR_20_5_to_15
# build two hypothetical cases and look at linear predictor difference
lp_160_5  <- predict(logit_add, newdata = data.frame(
countries = "160 of 192",
sanctions = "5%"
), type = "link")
lp_160_15 <- predict(logit_add, newdata = data.frame(
countries = "160 of 192",
sanctions = "15%"
), type = "link")
log_OR_160_5_to_15 <- lp_160_15 - lp_160_5
OR_160_5_to_15     <- exp(log_OR_160_5_to_15)
OR_160_5_to_15
# Make sure factors have the same levels as in the model
newdat_80_none <- data.frame(
countries = factor("80 of 192", levels = levels(climateSupport$countries)),
sanctions = factor("None",      levels = levels(climateSupport$sanctions))
)
p_80_none <- predict(logit_add, newdata = newdat_80_none, type = "response")
p_80_none
# Additive model
logit_add <- glm(
choice ~ countries + sanctions,
data   = climateSupport,
family = binomial(link = "logit")
)
# Model with interaction
logit_int <- glm(
choice ~ countries * sanctions,
data   = climateSupport,
family = binomial(link = "logit")
)
# Likelihood ratio test
anova(logit_add, logit_int, test = "Chisq")
# Additive model
logit_add <- glm(
choice ~ countries + sanctions,
data   = climateSupport,
family = binomial(link = "logit")
)
# Model with interaction
logit_int <- glm(
choice ~ countries * sanctions,
data   = climateSupport,
family = binomial(link = "logit")
)
# Likelihood ratio test
anova(logit_add, logit_int, test = "Chisq")
